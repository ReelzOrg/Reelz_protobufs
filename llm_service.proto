syntax = "proto3";

package llm;

message ContentBlock {
	string mime_type = 1; 
	oneof data {
		string text = 2;
		string file_uri = 3; // Don't send 50MB videos via gRPC directly. Send the S3 path or URL.
		//grpc max limit is 4MB, and while that can be increased, it's not recommended 
	}
	// For RAG source tracking
	// e.g., {"page_number": "12", "original_filename": "report.pdf"}
	map<string, string> metadata = 5;
}

message ChatMessage {
	string role = 1; // "user", "assistant"
	repeated ContentBlock content = 2;
}

message LLMConfig {
	optional float temperature = 1;
	optional int32 max_tokens = 2;
	optional int32 top_p = 3;
	optional int32 frequency_penalty = 4;
	optional int32 presence_penalty = 5;
	optional int32 top_k = 6;

	repeated string tools = 7; // repeated are already optional so no need for optional keyword
}

message LLMRequest {
	// because the model can send multimodal responses
	// if we want to aloow the user to set the system prompt then use the ChatMessage type instead of ContentBlock
	repeated ContentBlock prompt = 1;
	string model = 2;
	string user_id = 3;
	string session_id = 4;
	repeated ChatMessage history = 5;

	LLMConfig config = 6;
}

message LLMResponse {
	// because the model can send multimodal responses
	ContentBlock content = 1;

	// NEW: Finish reason (e.g., "stop", "length", "tool_calls")
	string finish_reason = 2;

	// To handle multi-block streaming (e.g. Text then Image)
	// 0 = first block, 1 = second block. 
	// If index changes, Client knows it's a new block, not an append.
	int32 content_index = 3;
	
	// For billing
	int32 input_tokens = 4;
	int32 output_tokens = 5;
}

service LLMService {
	rpc LLMChat (LLMRequest) returns (stream LLMResponse) {}
}